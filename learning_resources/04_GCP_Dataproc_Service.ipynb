{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUcjyDabALSN6X7iJHAJbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easycloudapi/learn_gcp/blob/main/learning_resources/04_GCP_Dataproc_Service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataproc Cluster"
      ],
      "metadata": {
        "id": "w9V1XPG_vpsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Pipeline 1: Preprocessing BigQuery Data with PySpark on Dataproc\n",
        "\n",
        "Ref: https://codelabs.developers.google.com/codelabs/pyspark-bigquery#1"
      ],
      "metadata": {
        "id": "pLzq9lV7vt9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Enabling the Compute Engine, Dataproc and BigQuery Storage APIs"
      ],
      "metadata": {
        "id": "Bq04bxfRvfzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "gcloud services enable compute.googleapis.com \\\n",
        "dataproc.googleapis.com \\\n",
        "bigquerystorage.googleapis.com\n",
        "```"
      ],
      "metadata": {
        "id": "SPlpR_ODvINg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Config The Environment"
      ],
      "metadata": {
        "id": "LQTtLDxewOcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "# Environment Variable Setup\n",
        "PROJECT_ID=test-flask-api-372913  \\\n",
        "REGION=us-central1  \\\n",
        "CLUSTER_NAME=demo-cluster-01 \\\n",
        "BUCKET_NAME=demo-$(date +%Y%m%d%H%m)\n",
        "```"
      ],
      "metadata": {
        "id": "zRL6dS4gw-EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "# Project Configuration Setup\n",
        "gcloud config set project $PROJECT_ID  \\\n",
        "gcloud config set dataproc/region $REGION\n",
        "```"
      ],
      "metadata": {
        "id": "QqzGaBAiwmtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create a Dataproc Cluster"
      ],
      "metadata": {
        "id": "03DrS3eowObF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "gcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n",
        "    --worker-machine-type n1-standard-2 \\\n",
        "    --master-boot-disk-size 50GB\n",
        "    --num-workers 2 \\\n",
        "    --worker-boot-disk-size 100GB\n",
        "    --image-version 1.5-debian \\\n",
        "    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n",
        "    --metadata 'PIP_PACKAGES=google-cloud-storage' \\\n",
        "    --optional-components=ANACONDA \\\n",
        "    --enable-component-gateway\n",
        "```"
      ],
      "metadata": {
        "id": "mZsPWbVFwOSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "# working code\n",
        "gcloud dataproc clusters create ${CLUSTER_NAME} --enable-component-gateway --region ${REGION} --master-machine-type n2-standard-2 --master-boot-disk-size 100 --num-workers 2 --worker-machine-type n2-standard-2 --worker-boot-disk-size 100 --image-version 2.1-debian11 --optional-components JUPYTER --scopes 'https://www.googleapis.com/auth/cloud-platform' --project test-flask-api-372913\n",
        "```"
      ],
      "metadata": {
        "id": "YFdnhrpDRGpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.1: Create a GCS Bucket\n",
        "\n",
        "```shell\n",
        "gsutil mb gs://${BUCKET_NAME}\n",
        "```"
      ],
      "metadata": {
        "id": "YbUGbGy23AgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Exploration In **Bigquery**\n"
      ],
      "metadata": {
        "id": "lT-u_1b--YI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "select * from fh-bigquery.reddit_posts.2017_01 limit 10;\n",
        "```"
      ],
      "metadata": {
        "id": "eTbMPdhj-eDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Clone **pyspark_bq_show_dataproc.py** from Repo And Submit the Job\n",
        "\n",
        "[Click Here to Get **pyspark_bq_show_dataproc.py** Code](https://github.com/easycloudapi/learn_gcp/blob/main/python_helpers/pyspark_bq_show_dataproc.py)"
      ],
      "metadata": {
        "id": "SUYxhFZbCbsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "# print as anoutput for \"subreddit name\" and \"count of each subreddit\"\n",
        "gcloud dataproc jobs submit pyspark --cluster ${CLUSTER_NAME} \\\n",
        "    --jars gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\n",
        "    --driver-log-levels root=FATAL \\\n",
        "    pyspark_bq_show_dataproc.py\n",
        "```"
      ],
      "metadata": {
        "id": "lmAbd6IiA8Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Clone **pyspark_bq_to_gcs_dataproc.py** from Repo And Submit the Job\n",
        "\n",
        "[Click Here to Get The **pyspark_bq_to_gcs_dataproc.py** Code](https://github.com/easycloudapi/learn_gcp/blob/main/python_helpers/pyspark_bq_to_gcs_dataproc.py)"
      ],
      "metadata": {
        "id": "wu0pDdM3AGkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```shell\n",
        "YEAR=2018 \\\n",
        "MONTH=12\n",
        "\n",
        "# Move data from BQ to GCS bucket\n",
        "gcloud dataproc jobs submit pyspark \\\n",
        "    --cluster ${CLUSTER_NAME} \\\n",
        "    --jars gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\n",
        "    --driver-log-levels root=FATAL \\\n",
        "    pyspark_bq_to_gcs_dataproc.py \\\n",
        "    -- ${YEAR} ${MONTH} ${BUCKET_NAME}\n",
        "```"
      ],
      "metadata": {
        "id": "KwdMI2SsTomA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Exploring the Dataproc and Spark UIs"
      ],
      "metadata": {
        "id": "DmKx3C4DDFLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataproc UI**\n",
        "\n",
        "![image](https://github.com/easycloudapi/learn_gcp/assets/108976294/81c846a1-0dd6-456a-9820-aee880069be0)"
      ],
      "metadata": {
        "id": "7AMqzOJqDIUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark UI**\n",
        "\n",
        "(Goto dataproc Clusters -> Click on cluster name -> Then click on \"**WEB INTERFACES**\")\n",
        "\n",
        "![image](https://github.com/easycloudapi/learn_gcp/assets/108976294/cf4b389e-7d15-411e-9651-ea490771c6fc)\n",
        "\n"
      ],
      "metadata": {
        "id": "0KwpAtr2FvGp"
      }
    }
  ]
}